---
title: 卷积神经网络简介
date: 2020-01-16 19:22:52
tags:
---
卷积神经网络（Convolutional Neural Network，CNN）是一种前馈神经网络。  
 它包括卷积层(convolutional layer)和池化层(pooling layer)。
 <!--more-->
 ![卷积神经网络与全连接神经网络](http://file.elecfans.com/web1/M00/4F/88/pIYBAFreggeAJhflAACFamG9M3o011.png)
 左图：全连接神经网络（平面），组成：输入层、激活函数、全连接层  
右图：卷积神经网络（立体），组成：输入层、**卷积层**、激活函数、**池化层**、全连接层   
  
# 直观理解卷积  
![](http://file.elecfans.com/web1/M00/4F/88/pIYBAFreggiATHJ7AAEstloH4_M280.png)
第一次卷积可以提取出低层次的特征。

第二次卷积可以提取出中层次的特征。

第三次卷积可以提取出高层次的特征。

# 卷积计算流程
![](http://file.elecfans.com/web1/M00/4F/88/pIYBAFreggiAVJYtAAEv0s5MlhM898.png)
左区域的三个大矩阵是原式图像的输入，RGB三个通道用三个矩阵表示，大小为7*7*3。

Filter W0表示1个filter助手，尺寸为3*3，深度为3（三个矩阵）；Filter W1也表示1个filter助手。因为卷积中我们用了2个filter，因此该卷积层结果的输出深度为2（绿色矩阵有2个）。

Bias b0是Filter W0的偏置项，Bias b1是Filter W1的偏置项。

OutPut是卷积后的输出，尺寸为3*3，深度为2。

# 详细计算过程
输入是固定的，filter是指定的，因此计算就是如何得到绿色矩阵。第一步，在输入矩阵上有一个和filter相同尺寸的滑窗，然后输入矩阵的在滑窗里的部分与filter矩阵对应位置相乘：
![](http://file.elecfans.com/web1/M00/4F/88/pIYBAFreggiAdu98AAA3JRRQkqA064.png)   与   ![](http://file.elecfans.com/web1/M00/4F/88/pIYBAFreggiAHbB9AAA0zhn7kn0788.png) 对应位置相乘求和，得0  

![](http://file.elecfans.com/web1/M00/4F/88/pIYBAFreggmAHKC1AAA8j3RmKH0221.png)与![](http://file.elecfans.com/web1/M00/4F/88/pIYBAFreggmADjKaAABBXoHbvrU697.png)对应位置相乘，得2  

![](http://file.elecfans.com/web1/M00/4F/88/pIYBAFreggmAB4JyAAA-tzT0MI0917.png) 与 ![](http://file.elecfans.com/web1/M00/4F/88/pIYBAFreggmAVgckAABFX3cm22o439.png)对应位置相乘，得0  
第二步，将3个矩阵产生的结果求和，并加上偏置项，即0+2+0+1=3，因此就得到了输出矩阵的左上角的3  
第三步，让每一个filter都执行这样的操作，变可得到第一个元素  
第四步，滑动窗口2个步长，重复之前步骤进行计算

# 池化层
![池化层](http://file.elecfans.com/web1/M00/4F/89/pIYBAFreggyACO9FAABorvOb-GE402.png)
池化就是对特征图进行特征压缩。池化也叫做下采样。选择原来某个区域的mean或max代替。  

![](http://file.elecfans.com/web1/M00/4F/89/pIYBAFreggyACkZ-AABzNVZsMiQ031.png)
>relu函数是人工智能的一种activation function

```math
f(x) = max(0, x)
```
![ReLU function](https://gss2.bdstatic.com/-fo3dSag_xI4khGkpoWK1HF6hhy/baike/w%3D268%3Bg%3D0/sign=4d2369e667600c33f079d9ce22773632/d788d43f8794a4c25b5e4dd902f41bd5ac6e39c6.jpg)

